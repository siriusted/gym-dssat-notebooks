{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SB3_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMN6nCGhxo0SPjzSftn6F1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siriusted/gym-dssat-notebooks/blob/master/SB3_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gym-DSSAT x Stable-Baselines3 Tutorial\n",
        "\n",
        "Welcome to a brief introduction to using gym-dssat with stable-baselines3.\n",
        "\n",
        "For a background or more details about using stable-baselines3 for reinforcement learning, please take a look [here](https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3)\n",
        "\n",
        "In this notebook, we will assume familiarity with reinforcement learning and stable-baselines3. Thus the focus is on interacting with gym-dssat using SB3.\n",
        "\n",
        "Next we proceed with installations\n",
        "\n",
        "# Installation\n",
        "- gym_dssat"
      ],
      "metadata": {
        "id": "10LKppXsZa-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"deb [ arch=amd64 ] https://raw.githubusercontent.com/pdidev/repo/ubuntu bionic main\" | sudo tee /etc/apt/sources.list.d/pdi.list > /dev/null\n",
        "!wget -O /etc/apt/trusted.gpg.d/pdidev-archive-keyring.gpg https://raw.githubusercontent.com/pdidev/repo/ubuntu/pdidev-archive-keyring.gpg\n",
        "!chmod a+r /etc/apt/trusted.gpg.d/pdidev-archive-keyring.gpg /etc/apt/sources.list.d/pdi.list\n",
        "!apt update &> /dev/null\n",
        "!apt install pdidev-archive-keyring libpdi-dev &> /dev/null\n",
        "\n",
        "!wget  http://gac.udc.es/~emilioj/bionic.tgz\n",
        "!tar -xf bionic.tgz\n",
        "!cd bionic/ && apt install `find . -name \"*.deb\"` &> /dev/null\n",
        "\n",
        "!pip install -U PyYAML &> /dev/null\n",
        "\n",
        "# add newly installed libraries to path\n",
        "import sys\n",
        "sys.path.append('/opt/gym_dssat_pdi/lib/python3.7/site-packages')"
      ],
      "metadata": {
        "id": "UxFEACyRZ162"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- stable_baselines3"
      ],
      "metadata": {
        "id": "vzq7QeYz3fbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra] &> /dev/null"
      ],
      "metadata": {
        "id": "uSyRCO333jgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All set! \n",
        "\n",
        "Next, we will train a PPO agent using stable-baselines3. This agent will be compared to two hardcoded agents, namely a Null agent and an Expert agent.\n",
        "\n",
        "We will use a wrapper below to make for easy interaction with SB3, as well as plotting utilities."
      ],
      "metadata": {
        "id": "N6Z_eE-8aUfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_dssat_pdi\n",
        "import numpy as np\n",
        "\n",
        "# helpers for action normalization\n",
        "def normalize_action(action_space_limits, action):\n",
        "    \"\"\"Normalize the action from [low, high] to [-1, 1]\"\"\"\n",
        "    low, high = action_space_limits\n",
        "    return 2.0 * ((action - low) / (high - low)) - 1.0\n",
        "\n",
        "def denormalize_action(action_space_limits, action):\n",
        "    \"\"\"Denormalize the action from [-1, 1] to [low, high]\"\"\"\n",
        "    low, high = action_space_limits\n",
        "    return low + (0.5 * (action + 1.0) * (high - low))\n",
        "\n",
        "# Wrapper for easy and uniform interfacing with SB3\n",
        "class GymDssatWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(GymDssatWrapper, self).__init__(env)\n",
        "\n",
        "        self.action_low, self.action_high = self._get_action_space_bounds()\n",
        "\n",
        "        # using a normalized action space\n",
        "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(1,), dtype=\"float32\")\n",
        "\n",
        "        # using a vector representation of observations to allow\n",
        "        # easily using SB3 MlpPolicy\n",
        "        self.observation_space = gym.spaces.Box(low=0.0,\n",
        "                                                high=np.inf,\n",
        "                                                shape=env.observation_dict_to_array(\n",
        "                                                    env.observation).shape,\n",
        "                                                dtype=\"float32\"\n",
        "                                                )\n",
        "        \n",
        "        # to avoid annoying problem with Monitor when episodes end and things are None\n",
        "        self.last_info = {}\n",
        "        self.last_obs = None\n",
        "\n",
        "    def _get_action_space_bounds(self):\n",
        "        box = self.env.action_space['anfer']\n",
        "        return box.low, box.high\n",
        "\n",
        "    def _format_action(self, action):\n",
        "        return { 'anfer': action[0] }\n",
        "\n",
        "    def _format_observation(self, observation):\n",
        "        return self.env.observation_dict_to_array(observation)\n",
        "\n",
        "    def reset(self):\n",
        "        return self._format_observation(self.env.reset())\n",
        "        \n",
        "\n",
        "    def step(self, action):\n",
        "        # Rescale action from [-1, 1] to original action space interval\n",
        "        denormalized_action = denormalize_action((self.action_low, self.action_high), action)\n",
        "        formatted_action = self._format_action(denormalized_action)\n",
        "        obs, reward, done, info = self.env.step(formatted_action)\n",
        "\n",
        "        # handle `None`s in obs, reward, and info on done step\n",
        "        if done:\n",
        "            obs, reward, info = self.last_obs, 0, self.last_info\n",
        "        else:\n",
        "            self.last_obs = obs\n",
        "            self.last_info = info\n",
        "\n",
        "        formatted_observation = self._format_observation(obs)\n",
        "        return formatted_observation, reward, done, info\n",
        "\n",
        "    def close(self):\n",
        "        return self.env.close()\n",
        "\n",
        "    def eval(self):\n",
        "        return self.env.set_evaluation()\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()"
      ],
      "metadata": {
        "id": "NlJ1N6earVmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will create the environment and train the PPO agent..."
      ],
      "metadata": {
        "id": "xBP1l9AFp5dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "# Create environment\n",
        "env_args = {\n",
        "    'run_dssat_location': '/opt/dssat_pdi/run_dssat',\n",
        "    'mode': 'fertilization',\n",
        "    'seed': 123,\n",
        "    'random_weather': True,\n",
        "}\n",
        "\n",
        "env = Monitor(GymDssatWrapper(gym.make('GymDssatPdi-v0', **env_args)))\n",
        "\n",
        "# Training arguments for PPO agent\n",
        "ppo_args = {\n",
        "    'seed': 123, # seed training for reproducibility\n",
        "}\n",
        "\n",
        "# Create the agent\n",
        "ppo_agent = PPO('MlpPolicy', env, **ppo_args)\n",
        "\n",
        "# Train for 10k timesteps\n",
        "print('Training PPO agent...')\n",
        "ppo_agent.learn(total_timesteps=1_000_000)\n",
        "print('Training done')"
      ],
      "metadata": {
        "id": "lSElUXBegk4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will compare to the baseline agents and display the results...."
      ],
      "metadata": {
        "id": "WI0XHJPEYBf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline agents for comparison\n",
        "class NullAgent:\n",
        "    \"\"\"\n",
        "    Agent always choosing to do no fertilization\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    def predict(self, obs, state=None, episode_start=None, deterministic=None):\n",
        "        action = normalize_action((self.env.action_low, self.env.action_high), [0])\n",
        "        return np.array([action], dtype=np.float32), obs\n",
        "\n",
        "\n",
        "class ExpertAgent:\n",
        "    \"\"\"\n",
        "    Simple agent using policy of choosing fertilization amount based on days after planting\n",
        "    \"\"\"\n",
        "    fertilization_dic = {\n",
        "        40: 27,\n",
        "        45: 35,\n",
        "        80: 54,\n",
        "    }\n",
        "\n",
        "    def __init__(self, env, normalize_action=False, fertilization_dic=None):\n",
        "        self.env = env\n",
        "        self.normalize_action = normalize_action\n",
        "\n",
        "    def _policy(self, obs):\n",
        "        dap = int(obs[0][0])\n",
        "        return [self.fertilization_dic[dap] if dap in self.fertilization_dic else 0]\n",
        "\n",
        "    def predict(self, obs, state=None, episode_start=None, deterministic=None):\n",
        "        action = self._policy(obs)\n",
        "        action = normalize_action((self.env.action_low, self.env.action_high), action)\n",
        "\n",
        "        return np.array([action], dtype=np.float32), obs"
      ],
      "metadata": {
        "id": "SFrrbyS6YQrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "#evaluation and plotting functions\n",
        "def evaluate(agent, n_episodes=10):\n",
        "    # Create eval env\n",
        "    eval_args = {\n",
        "        'run_dssat_location': '/opt/dssat_pdi/run_dssat',\n",
        "        'mode': 'fertilization',\n",
        "        'seed': 456,\n",
        "        'random_weather': True,\n",
        "    }\n",
        "    env = Monitor(GymDssatWrapper(gym.make('GymDssatPdi-v0', **eval_args)))\n",
        "\n",
        "    env.eval()\n",
        "\n",
        "    returns, _ = evaluate_policy(\n",
        "        agent, env, n_eval_episodes=n_episodes, return_episode_rewards=True)\n",
        "    \n",
        "    env.close()\n",
        "\n",
        "    return returns\n",
        "\n",
        "# evaluate agents\n",
        "null_agent = NullAgent(env)\n",
        "print('Evaluating Null agent...')\n",
        "null_returns = evaluate(null_agent)\n",
        "print('Done')\n",
        "print('Evaluating PPO agent...')\n",
        "ppo_returns = evaluate(ppo_agent)\n",
        "print('Done')\n",
        "\n",
        "expert_agent = ExpertAgent(env)\n",
        "print('Evaluating Expert agent...')\n",
        "expert_returns = evaluate(expert_agent)\n",
        "print('Done')\n",
        "\n",
        "# write results to a file to be loaded for display\n",
        "data = [('null', null_returns), ('ppo', ppo_returns), ('expert', expert_returns)]"
      ],
      "metadata": {
        "id": "7Tb6NmyRYZNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_results(data):\n",
        "    data_dict = {}\n",
        "    for label, returns in data:\n",
        "        data_dict[label] = returns\n",
        "    df = pd.DataFrame(data_dict)\n",
        "    \n",
        "    ax = sns.boxplot(data=df)\n",
        "    ax.set_xlabel(\"policy\")\n",
        "    ax.set_ylabel(\"evaluation output\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WU7YCK5ymV4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(data)"
      ],
      "metadata": {
        "id": "4ikz6VFLmzuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All done! Go ahead and edit and re-run the code cells above to observe results"
      ],
      "metadata": {
        "id": "AMTb6PMOuh53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus: Saving best performing model\n",
        "\n",
        "In this section, we will use a feature from stable-baselines3 known as `callbacks`. They enable different kind of operations during the training of a model. For our purpose here, we will use the `EvalCallback` to periodically evaluate and save the best performing model we find during training.\n",
        "\n",
        "For more on callbacks in stable-baselines3 see [here](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html)"
      ],
      "metadata": {
        "id": "l-n-EDrbmeoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "env = Monitor(GymDssatWrapper(gym.make('GymDssatPdi-v0', **env_args)))\n",
        "\n",
        "\n",
        "# Create the agent\n",
        "ppo_agent = PPO('MlpPolicy', env, **ppo_args)\n",
        "\n",
        "# path to save best model found\n",
        "path = 'ppo'\n",
        "\n",
        "# eval callback\n",
        "eval_freq = 1000\n",
        "eval_env_args = {**env_args, 'seed': 345}\n",
        "eval_env = Monitor(GymDssatWrapper(gym.make('GymDssatPdi-v0', **eval_env_args)))\n",
        "eval_callback = EvalCallback(eval_env,\n",
        "                                eval_freq=eval_freq,\n",
        "                                best_model_save_path=f'./{path}',\n",
        "                                deterministic=True)\n",
        "\n",
        "\n",
        "# Train\n",
        "print('Training PPO agent...')\n",
        "ppo_agent.learn(total_timesteps=1_000_000, callback=eval_callback)\n",
        "ppo_agent.save(f'./{path}/final_model')\n",
        "print('Training done')"
      ],
      "metadata": {
        "id": "6zXv0O6Vmfsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate agents\n",
        "null_agent = NullAgent(env)\n",
        "print('Evaluating Null agent...')\n",
        "null_returns = evaluate(null_agent)\n",
        "print('Done')\n",
        "\n",
        "print('Evaluating PPO agents...')\n",
        "ppo_best = PPO.load(f'./{path}/best_model')\n",
        "ppo_best_returns = evaluate(ppo_best)\n",
        "ppo_returns = evaluate(ppo_agent)\n",
        "print('Done')\n",
        "\n",
        "expert_agent = ExpertAgent(env)\n",
        "print('Evaluating Expert agent...')\n",
        "expert_returns = evaluate(expert_agent)\n",
        "print('Done')\n",
        "\n",
        "\n",
        "results = [('null', null_returns), ('ppo(best)', ppo_best_returns),  ('ppo(final)', ppo_returns), ('expert', expert_returns)]"
      ],
      "metadata": {
        "id": "0OYODqkbmltX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(results)"
      ],
      "metadata": {
        "id": "zks9sRZNmq74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0oWsqeDsm65O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}