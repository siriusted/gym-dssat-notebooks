{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10LKppXsZa-N"
   },
   "source": [
    "# Gym-DSSAT x Stable-Baselines3 Tutorial\n",
    "\n",
    "Welcome to a brief introduction to using gym-dssat with stable-baselines3.\n",
    "\n",
    "In this tutorial, we will assume familiarity with reinforcement learning and stable-baselines3.\n",
    "\n",
    "For a background or more details about using stable-baselines3 for reinforcement learning, please take a look [here](https://stable-baselines3.readthedocs.io/en/master/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlJ1N6earVmd"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_dssat_pdi\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6Z_eE-8aUfG"
   },
   "source": [
    "All set! \n",
    "\n",
    "Next, we will train a PPO agent using stable-baselines3. This agent will be compared to two hardcoded agents, namely a Null agent and an Expert agent.\n",
    "\n",
    "To ease interfacing with stable-baselines3, we will create a wrapper around gym-dssat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAqcdMoc8Yo_"
   },
   "outputs": [],
   "source": [
    "# helpers for action normalization\n",
    "def normalize_action(action_space_limits, action):\n",
    "    \"\"\"Normalize the action from [low, high] to [-1, 1]\"\"\"\n",
    "    low, high = action_space_limits\n",
    "    return 2.0 * ((action - low) / (high - low)) - 1.0\n",
    "\n",
    "def denormalize_action(action_space_limits, action):\n",
    "    \"\"\"Denormalize the action from [-1, 1] to [low, high]\"\"\"\n",
    "    low, high = action_space_limits\n",
    "    return low + (0.5 * (action + 1.0) * (high - low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbIMwVcAa6Zy"
   },
   "outputs": [],
   "source": [
    "class GymDssatWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(GymDssatWrapper, self).__init__(env)\n",
    "\n",
    "        self.action_low, self.action_high = self._get_action_space_bounds()\n",
    "\n",
    "        # using a normalized action space\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(1,), dtype=\"float32\")\n",
    "\n",
    "        # using a vector representation of observations to allow\n",
    "        # easily using SB3 MlpPolicy\n",
    "        self.observation_space = gym.spaces.Box(low=0.0,\n",
    "                                                high=np.inf,\n",
    "                                                shape=env.observation_dict_to_array(\n",
    "                                                    env.observation).shape,\n",
    "                                                dtype=\"float32\"\n",
    "                                                )\n",
    "        \n",
    "        # to avoid annoying problem with Monitor when episodes end and things are None\n",
    "        self.last_info = {}\n",
    "        self.last_obs = None\n",
    "\n",
    "    def _get_action_space_bounds(self):\n",
    "        box = self.env.action_space['anfer']\n",
    "        return box.low, box.high\n",
    "\n",
    "    def _format_action(self, action):\n",
    "        return { 'anfer': action[0] }\n",
    "\n",
    "    def _format_observation(self, observation):\n",
    "        return self.env.observation_dict_to_array(observation)\n",
    "\n",
    "    def reset(self):\n",
    "        return self._format_observation(self.env.reset())\n",
    "        \n",
    "\n",
    "    def step(self, action):\n",
    "        # Rescale action from [-1, 1] to original action space interval\n",
    "        denormalized_action = denormalize_action((self.action_low, self.action_high), action)\n",
    "        formatted_action = self._format_action(denormalized_action)\n",
    "        obs, reward, done, info = self.env.step(formatted_action)\n",
    "\n",
    "        # handle `None`s in obs, reward, and info on done step\n",
    "        if done:\n",
    "            obs, reward, info = self.last_obs, 0, self.last_info\n",
    "        else:\n",
    "            self.last_obs = obs\n",
    "            self.last_info = info\n",
    "\n",
    "        formatted_observation = self._format_observation(obs)\n",
    "        return formatted_observation, reward, done, info\n",
    "\n",
    "    def close(self):\n",
    "        return self.env.close()\n",
    "\n",
    "    def seed(self, seed):\n",
    "        self.env.set_seed(seed)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBP1l9AFp5dy"
   },
   "source": [
    "Train a PPO agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRNZq0TbqAVt"
   },
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env_args = {\n",
    "  'run_dssat_location': '/opt/dssat_pdi/run_dssat',\n",
    "  'mode': 'fertilization',\n",
    "  'seed': 123,\n",
    "  'random_weather': True,\n",
    "}\n",
    "\n",
    "env = GymDssatWrapper(gym.make('GymDssatPdi-v0', **env_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments for PPO agent\n",
    "ppo_args = {\n",
    "    'batch_size': 128,\n",
    "    'n_steps': 256,\n",
    "    'gamma': 0.95,\n",
    "    'learning_rate': 0.003,\n",
    "    'clip_range': 0.1,\n",
    "    'n_epochs': 20,\n",
    "    'policy_kwargs': dict(\n",
    "        net_arch=[dict(pi=[64, 64], vf=[64, 64])],\n",
    "        activation_fn=torch.nn.Tanh,\n",
    "        ortho_init=False,\n",
    "    ),\n",
    "    'seed': 123,\n",
    "}\n",
    "\n",
    "# Create the agent\n",
    "ppo_agent = PPO('MlpPolicy', env, **ppo_args)\n",
    "\n",
    "# Train for 40k timesteps\n",
    "ppo_agent.learn(total_timesteps=40_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IabwIrKobeVv"
   },
   "source": [
    "Below are the agents we will compare our trained agent to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9ql7FBEb3MK"
   },
   "outputs": [],
   "source": [
    "class NullAgent:\n",
    "    \"\"\"\n",
    "    Agent always choosing to do no fertilization\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def predict(self, obs, state=None, deterministic=None):\n",
    "        action = normalize_action((self.env.action_low, self.env.action_high), [0])\n",
    "        return np.array([action], dtype=np.float32), obs\n",
    "\n",
    "\n",
    "class ExpertAgent:\n",
    "    \"\"\"\n",
    "    Simple agent using policy of choosing fertilization amount based on days after planting\n",
    "    \"\"\"\n",
    "    fertilization_dic = {\n",
    "        40: 27,\n",
    "        45: 35,\n",
    "        80: 54,\n",
    "    }\n",
    "\n",
    "    def __init__(self, env, normalize_action=False, fertilization_dic=None):\n",
    "        self.env = env\n",
    "        self.normalize_action = normalize_action\n",
    "\n",
    "    def _policy(self, obs):\n",
    "        dap = int(obs[0][0])\n",
    "        return [self.fertilization_dic[dap] if dap in self.fertilization_dic else 0]\n",
    "\n",
    "    def predict(self, obs, state=None, deterministic=None):\n",
    "        action = self._policy(obs)\n",
    "        action = normalize_action((self.env.action_low, self.env.action_high), action)\n",
    "\n",
    "        return np.array([action], dtype=np.float32), obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZPVO2ZEb0bm"
   },
   "source": [
    "We will also create helpers for evaluating the three agents and plotting results.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcgIRQWrcGXP"
   },
   "outputs": [],
   "source": [
    "def evaluate(agent, n_episodes=10):\n",
    "    # Create eval env\n",
    "    eval_args = {\n",
    "        'run_dssat_location': '/opt/dssat_pdi/run_dssat',\n",
    "        'mode': 'fertilization',\n",
    "        'seed': 456,\n",
    "        'random_weather': True,\n",
    "    }\n",
    "    env = Monitor(GymDssatWrapper(gym.make('GymDssatPdi-v0', **eval_args)))\n",
    "\n",
    "    returns, _ = evaluate_policy(\n",
    "        agent, env, n_eval_episodes=n_episodes, return_episode_rewards=True)\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    return returns\n",
    "\n",
    "def plot_results(labels, returns):\n",
    "    data_dict = {}\n",
    "    for label, data in zip(labels, returns):\n",
    "        data_dict[label] = data\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    ax = sns.boxplot(data=df)\n",
    "    ax.set_xlabel(\"policy\")\n",
    "    ax.set_ylabel(\"evaluation output\")\n",
    "    plt.savefig('results.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsMEvflDsvPC"
   },
   "source": [
    "Evaluate Null agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLV7gy30tdsH"
   },
   "outputs": [],
   "source": [
    "null_agent = NullAgent(env)\n",
    "null_returns = evaluate(null_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDXSXpmEtwQf"
   },
   "source": [
    "Evaluate PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlbcxo9Et2dP"
   },
   "outputs": [],
   "source": [
    "ppo_returns = evaluate(ppo_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXaKWzrft6l_"
   },
   "source": [
    "Evaluate Expert agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCiIZrAGt-UH"
   },
   "outputs": [],
   "source": [
    "expert_agent = ExpertAgent(env)\n",
    "expert_returns = evaluate(expert_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VvbyTlruaY9"
   },
   "source": [
    "\n",
    "\n",
    "Finally, we display the results in the following plot. A copy is also saved as results.pdf in the current working directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hafYy2uZudt9"
   },
   "outputs": [],
   "source": [
    "labels = ['null', 'ppo', 'expert']\n",
    "returns = [null_returns, ppo_returns, expert_returns]\n",
    "plot_results(labels, returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMTb6PMOuh53"
   },
   "source": [
    "And, there you have it! Now, you can go ahead and keep playing with gym-dssat and stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXzd0kDfunVx"
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SB3_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
